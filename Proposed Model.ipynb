{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import matplotlib\n",
    "import torch\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.init as init\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.utils.data as Data\n",
    "import matplotlib.dates as mdates\n",
    "from torch.autograd import Variable\n",
    "import time\n",
    "import datetime\n",
    "import sklearn.metrics as sk\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import calendar\n",
    "import numpy as np\n",
    "import joblib\n",
    "import sys\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "from tqdm.notebook import trange, tqdm\n",
    "\n",
    "device0 = torch.device('cuda' if torch.cuda.is_available else 'cpu')\n",
    "np.random.seed(16)\n",
    "torch.manual_seed(16)\n",
    "torch.cuda.manual_seed(16) \n",
    "torch.set_default_dtype(torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "X_spt=joblib.load('X_cov')\n",
    "Z=joblib.load('Data_nonGaussian')\n",
    "# Z=joblib.load('Data_Gaussian')\n",
    "X_spt=X_spt.float().cuda()\n",
    "Z=Z.float().cuda()\n",
    "T=1095\n",
    "S=11025\n",
    "Z=Z.T\n",
    "Z.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVD decomposition of training data\n",
    "def SVD(Z_train):\n",
    "    Z_tilde=Z_train-torch.mean(Z_train)\n",
    "    u,s,vt=torch.linalg.svd(Z_tilde,full_matrices=False)\n",
    "    return(u,s,vt)\n",
    "\n",
    "def recompose(u,s,vt):\n",
    "    tmp=u @ torch.diag(s) @ vt\n",
    "    return(tmp)\n",
    "\n",
    "# convert index from test_y_pred to coordinate\n",
    "def convert(a):\n",
    "    y=(a+9026)//105-1\n",
    "    x=(a+9026)%105-1\n",
    "    return x,y\n",
    "\n",
    "# Model\n",
    "# dim_output should be k=min(n,T)\n",
    "\n",
    "class Net(nn.Module): \n",
    "    def __init__(self,dim_output, dim_input=11, dim_emb=64,dim_linear=128, dropout_input=0.8, dropout_emb=0.5, \n",
    "                 dropout_context=0.5,  l2=0.0001, batch_first=True):\n",
    "        super(Net,self).__init__()\n",
    "        self.batch_first = batch_first\n",
    "        self.embedding = nn.Sequential(\n",
    "            nn.Dropout(p=dropout_input),\n",
    "            nn.Linear(dim_input, dim_emb, bias=False),\n",
    "            nn.Dropout(p=dropout_emb)\n",
    "        )\n",
    "        init.xavier_normal_(self.embedding[1].weight)\n",
    "        \n",
    "        \n",
    "        self.input = nn.Linear(dim_emb,dim_linear,bias=True)\n",
    "        init.xavier_normal_(self.input.weight)\n",
    "        self.input.bias.data.zero_()\n",
    "        \n",
    "        self.hidden1 = nn.Linear(dim_linear,2*dim_linear,bias=True)\n",
    "        init.xavier_normal_(self.hidden1.weight)\n",
    "        self.hidden1.bias.data.zero_()\n",
    "        \n",
    "        self.hidden2 = nn.Linear(2*dim_linear,2*dim_linear,bias=True)\n",
    "        init.xavier_normal_(self.hidden2.weight)\n",
    "        self.hidden2.bias.data.zero_()\n",
    "        \n",
    "        self.hidden3 = nn.Linear(2*dim_linear,2*dim_linear,bias=True)\n",
    "        init.xavier_normal_(self.hidden3.weight)\n",
    "        self.hidden3.bias.data.zero_()\n",
    "        \n",
    "        self.hidden4 = nn.Linear(2*dim_linear,2*dim_linear,bias=True)\n",
    "        init.xavier_normal_(self.hidden4.weight)\n",
    "        self.hidden4.bias.data.zero_()\n",
    "        \n",
    "        self.hidden5 = nn.Linear(2*dim_linear,dim_linear,bias=True)\n",
    "        init.xavier_normal_(self.hidden5.weight)\n",
    "        self.hidden5.bias.data.zero_()\n",
    "        \n",
    "        self.outputdnn = nn.Linear(dim_linear,dim_emb,bias=True)\n",
    "        init.xavier_normal_(self.outputdnn.weight)\n",
    "        self.outputdnn.bias.data.zero_()\n",
    "        \n",
    "        self.attention = nn.Linear(dim_emb,dim_emb,bias=True)\n",
    "        init.xavier_normal_(self.attention.weight, gain=nn.init.calculate_gain('tanh'))\n",
    "        self.attention.bias.data.zero_()\n",
    "        \n",
    "        \n",
    "        self.output = nn.Sequential(\n",
    "            nn.Dropout(p=dropout_context),\n",
    "            nn.Linear(in_features=dim_emb, out_features=dim_output)\n",
    "        )\n",
    "        init.xavier_normal_(self.output[1].weight)\n",
    "        self.output[1].bias.data.zero_()\n",
    "        \n",
    "        self.input2 = nn.Linear(dim_emb,dim_linear,bias=True)\n",
    "        init.xavier_normal_(self.input2.weight)\n",
    "        self.input2.bias.data.zero_()\n",
    "\n",
    "        \n",
    "        self.hidden6 = nn.Linear(dim_linear,2*dim_linear,bias=True)\n",
    "        init.xavier_normal_(self.hidden6.weight)\n",
    "        self.hidden6.bias.data.zero_()\n",
    "        \n",
    "        self.hidden7 = nn.Linear(2*dim_linear,2*dim_linear,bias=True)\n",
    "        init.xavier_normal_(self.hidden7.weight)\n",
    "        self.hidden7.bias.data.zero_()\n",
    "        \n",
    "        self.hidden8 = nn.Linear(2*dim_linear,2*dim_linear,bias=True)\n",
    "        init.xavier_normal_(self.hidden8.weight)\n",
    "        self.hidden8.bias.data.zero_()\n",
    "        \n",
    "        self.hidden9 = nn.Linear(2*dim_linear,dim_linear,bias=True)\n",
    "        init.xavier_normal_(self.hidden9.weight)\n",
    "        self.hidden9.bias.data.zero_()\n",
    "        \n",
    "        self.outputcoef = nn.Linear(dim_linear,dim_input,bias=True)\n",
    "        init.xavier_normal_(self.outputcoef.weight)\n",
    "        self.outputcoef.bias.data.zero_()\n",
    "        \n",
    "\n",
    "    def forward(self,x):\n",
    "        \n",
    "        emb = self.embedding(x)\n",
    "        \n",
    "        \n",
    "        input0 = self.input(emb)\n",
    "        input0 = F.relu(input0)\n",
    "        \n",
    "        input1 = self.hidden1(input0)\n",
    "        input1 = F.relu(input1)\n",
    "        \n",
    "        input2 = self.hidden2(input1)\n",
    "        input2 = F.relu(input2)\n",
    "        \n",
    "        input3 = self.hidden3(input2)\n",
    "        input3 = F.relu(input3)\n",
    "        \n",
    "        input4 = self.hidden4(input3)\n",
    "        input4 = F.relu(input4)\n",
    "        \n",
    "        input5 = self.hidden5(input4)\n",
    "        input5 = F.relu(input5)\n",
    "        \n",
    "        output1 = self.outputdnn(input5)\n",
    "        \n",
    "        gamma = torch.tanh(self.attention(output1))\n",
    "        \n",
    "        context =  gamma*emb\n",
    "        \n",
    "        coefficient = self.output(context)\n",
    "        \n",
    "        input_cov = self.input2(emb)\n",
    "        input_cov = F.relu(input_cov)\n",
    "        \n",
    "        input6 = self.hidden6(input_cov)\n",
    "        input6 = F.relu(input6)\n",
    "        \n",
    "        input7 = self.hidden7(input6)\n",
    "        input7 = F.relu(input7)\n",
    "        \n",
    "        input8 = self.hidden8(input7)\n",
    "        input8 = F.relu(input8)\n",
    "        \n",
    "        input9 = self.hidden9(input8)\n",
    "        input9 = F.relu(input9)\n",
    "        \n",
    "        output_coef=self.outputcoef(input9)\n",
    "        \n",
    "        intercept = output_coef*x\n",
    "        \n",
    "        intercept = torch.sum(intercept,1,keepdim=True)\n",
    "        \n",
    "        finaltemp = recompose(coefficient,s,vt) + intercept + spatial_mean\n",
    "        \n",
    "        return finaltemp,gamma,output_coef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dim_output should be k=min(n,T)\n",
    "net = Net(dim_output=1095).cuda()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data set\n",
    "\n",
    "train_set = Data.TensorDataset(X_spt[:7025],Z[:7025,:])\n",
    "train_loader=Data.DataLoader(dataset=train_set, batch_size=8000,shuffle=False)\n",
    "\n",
    "valid_set = Data.TensorDataset(X_spt[7025:9025],Z[7025:9025,:])\n",
    "valid_loader=Data.DataLoader(dataset=valid_set, batch_size=2000,shuffle=False)\n",
    "\n",
    "test_set = Data.TensorDataset(X_spt[9025:],Z[9025:,:])\n",
    "test_loader =Data.DataLoader(dataset=test_set, batch_size=2000,shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset=Z[:7025,:]\n",
    "spatial_mean=torch.mean(train_dataset,0,True)\n",
    "u,s,vt=SVD(train_dataset)\n",
    "\n",
    "\n",
    "optimizer = torch.optim.Adam(net.parameters(),lr = 0.0001)\n",
    "# optimizer = torch.optim.SGD(net.parameters(),lr =0.0001,momentum=0.9,nesterov=True)\n",
    "loss_func = torch.nn.MSELoss()\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=8000, eta_min=0, last_epoch=-1, verbose=False)\n",
    "\n",
    "def epoch(loader,train=False):\n",
    "    if train:\n",
    "        net.train()\n",
    "        mode = 'Train'\n",
    "    else:\n",
    "        net.eval()\n",
    "        mode = 'Eval'\n",
    "    \n",
    "    true_value = []\n",
    "    outputs = []\n",
    "    attention =[]\n",
    "    output_coefs = []\n",
    "    losses = 0\n",
    "\n",
    "    for step ,(batch_x,batch_z) in enumerate(loader):\n",
    "        input_var=Variable(batch_x)\n",
    "        target_var=Variable(batch_z)\n",
    "    \n",
    "        prediction,att,output_coef = net(input_var)\n",
    "        loss = loss_func(prediction,target_var)\n",
    "        \n",
    "        attention.append(att.data)\n",
    "        outputs.append(prediction.data)\n",
    "        output_coefs.append(output_coef.data)\n",
    "        losses=losses+loss.data\n",
    "    \n",
    "        # compute gradient and do update step\n",
    "        if train:\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "    return torch.cat(outputs, 0),torch.cat(attention,0),torch.cat(output_coefs,0),losses/len(loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main\n",
    "best_valid_epoch = 0\n",
    "best_valid_loss = 1e10\n",
    "\n",
    "\n",
    "train_losses = []\n",
    "valid_losses = []\n",
    "n_epochs=8000\n",
    "\n",
    "time_start=time.time()\n",
    "\n",
    "for ei in trange(n_epochs):\n",
    "                \n",
    "    train_y_pred, train_att, train_coef, train_loss = epoch(train_loader,train=True)\n",
    "\n",
    "    train_losses.append(train_loss)\n",
    "\n",
    "#     Eval\n",
    "    valid_y_pred,valid_att,valid_coef, valid_loss = epoch(valid_loader)\n",
    "    valid_losses.append(valid_loss)\n",
    "\n",
    "    valid_y_pred = valid_y_pred.cpu()\n",
    "\n",
    "\n",
    "    is_best = valid_loss < best_valid_loss\n",
    "\n",
    "    if is_best:\n",
    "        best_valid_epoch = ei\n",
    "        best_valid_loss = valid_loss\n",
    "\n",
    "    # evaluate on the test set\n",
    "        test_y_pred,test_att,test_coef, test_loss = epoch(test_loader)\n",
    "        \n",
    "        train_coef_best = train_coef\n",
    "        train_att_best = train_att\n",
    "        train_y_pred_best = train_y_pred.cpu()\n",
    "        test_y_pred = test_y_pred.cpu()\n",
    "        \n",
    "        \n",
    "        with open('train_nonGaussian.txt', 'w') as f:\n",
    "            f.write('Best Validation Epoch: {}\\n'.format(ei))\n",
    "            f.write('Best Validation Loss: {}\\n'.format(best_valid_loss))\n",
    "            f.write('Train Loss: {}\\n'.format(train_loss))\n",
    "            f.write('Test Loss: {}\\n'.format(test_loss))\n",
    "\n",
    "   \n",
    "        torch.save(net,'nonGaussian.pt')\n",
    "    \n",
    "    scheduler.step()\n",
    "\n",
    "time_end=time.time()\n",
    "print('Run Time (s):' ,time_end-time_start)\n",
    "print('Run Time per Epoch (s):'(time_end-time_start)/n_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bestmodel=torch.load('nonGaussian.pt')\n",
    "paras = list(bestmodel.parameters())\n",
    "pred_data,att,coef=bestmodel(X_spt)\n",
    "print(bestmodel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"font.family\": 'sans-serif',\n",
    "    \"font.sans-serif\": ['Arial'],\n",
    "    \"font.size\": 20,\n",
    "    \"mathtext.fontset\": 'stixsans',\n",
    "}\n",
    "plt.rcParams.update(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check loss convergence\n",
    "for i in range(len(train_losses)):\n",
    "    train_losses[i]=train_losses[i].cpu()\n",
    "    valid_losses[i]=valid_losses[i].cpu()\n",
    "\n",
    "x=[i for i in range(len(train_losses))]\n",
    "plt.figure(figsize=(15,10))\n",
    "plt.plot(x,np.array(train_losses),linewidth = 3,color=\"red\",label='Training Loss')\n",
    "plt.plot(x,np.array(valid_losses),linewidth = 3,color=\"blue\",label='Validation Loss')\n",
    "# plt.title(r'$\\gamma$')\n",
    "ax=plt.gca()\n",
    "ax.set_ylabel(r'MSE')\n",
    "ax.set_xlabel('Epoch')\n",
    "plt.ylim(-0.02,0.3)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute PI'\n",
    "\n",
    "n=len(train_set)\n",
    "decay=0.0001\n",
    "def enable_dropout(model):\n",
    "    \"\"\" Function to enable the dropout layers during test-time \"\"\"\n",
    "    for m in model.modules():\n",
    "        if m.__class__.__name__.startswith('Dropout'):\n",
    "            m.train()\n",
    "            \n",
    "def uncertainity_estimate(x, model, num_samples, l2):\n",
    "    model.eval()\n",
    "    enable_dropout(model)\n",
    "    outputs=np.stack([model(x)[0].cpu().detach().numpy() for _ in range(num_samples)]).reshape(num_samples,1095).T\n",
    "    Z_mean=outputs.mean(axis=1)\n",
    "    Z_variance=outputs.var(axis=1)\n",
    "    tau = np.square(l2) * (1. - 0.5) / (2. * n *decay )\n",
    "    Z_variance += (1. / tau)\n",
    "    Z_std = np.sqrt(Z_variance)\n",
    "\n",
    "    return Z_mean, Z_std\n",
    "\n",
    "Z_mean,Z_std = uncertainity_estimate(X_spt[10659].reshape(1,len(X_spt[10659])),bestmodel,1000,0.1)\n",
    "Z_upper=test_y_pred[1634]+1.96*Z_std\n",
    "Z_lower=test_y_pred[1634]-1.96*Z_std\n",
    "Z_var=np.square(Z_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate map\n",
    "simu_pred=pred_data.cpu().detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_y_true=Z[9025:,:].cpu()\n",
    "\n",
    "loc_x, loc_y=10,88\n",
    "t=487\n",
    "def timeconvert_week(t):\n",
    "    return((t+1)/7-1)\n",
    "\n",
    "def timeconvert_month(t):\n",
    "    return((t+1)/30-1)\n",
    "\n",
    "true_map=Z[:,t].reshape(105,105).cpu()\n",
    "simu_map=simu_pred[:,t].reshape(105,105)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(24,16))\n",
    "\n",
    "plt.subplot(3,1,1)\n",
    "time=[]\n",
    "x=range(1095)\n",
    "plt.plot(x,np.array(test_y_true[1634,:]),linewidth = 3,color=\"orangered\",label='Test Data')\n",
    "plt.plot(x,np.array(test_y_pred[1634,:]),linewidth = 3,color=\"dodgerblue\",label='Prediction')\n",
    "plt.fill_between(x,Z_lower,Z_upper,color=\"dodgerblue\",alpha=0.3,label='95% PI')\n",
    "plt.title('Average daily temperature')\n",
    "ax=plt.gca()\n",
    "plt.axvline(t,linewidth = 3,linestyle = \"dashed\",color=\"black\")\n",
    "ax.set_ylabel(r'Temperature ($^{\\circ}$C)')\n",
    "plt.legend(loc = 'upper right')\n",
    "\n",
    "plt.subplot(3,1,2)\n",
    "x=range(156)\n",
    "true_week=[np.mean(np.array(test_y_true[1634,:])[i*7:(i+1)*7]) for i in range(156)]\n",
    "pred_week=[np.mean(np.array(test_y_pred[1634,:])[i*7:(i+1)*7]) for i in range(156)]\n",
    "vari_week=[np.sum(np.array(Z_var)[i*7:(i+1)*7])/49 for i in range(156)]\n",
    "pred_week_upper=pred_week+1.96*np.sqrt(vari_week)\n",
    "pred_week_lower=pred_week-1.96*np.sqrt(vari_week)\n",
    "plt.title('Average weekly temperature')\n",
    "plt.plot(x,true_week,linewidth = 3,color=\"orangered\",label='Test Data')\n",
    "plt.plot(x,pred_week,linewidth = 3,color=\"dodgerblue\",label='Prediction')\n",
    "plt.fill_between(x,pred_week_lower,pred_week_upper,color=\"dodgerblue\",alpha=0.3,label='95% PI')\n",
    "ax=plt.gca()\n",
    "plt.axvline(69,linewidth = 3,linestyle = \"dashed\",color=\"black\")\n",
    "ax.set_ylabel(r'Temperature ($^{\\circ}$C)')\n",
    "plt.legend(loc = 'upper right')\n",
    "\n",
    "plt.subplot(3,1,3)\n",
    "true_month=[]\n",
    "pred_month=[]\n",
    "vari_month=[]\n",
    "x=[datetime.date(2010,1,1)+relativedelta(months=i) for i in range(36)]\n",
    "y=range(36)\n",
    "for i in range(36):\n",
    "    if (i+1)%12==0:\n",
    "        true_month.append(np.mean(np.array(test_y_true[1634,:])[(x[i]-x[0]).days:(x[i]-x[0]).days+31]))\n",
    "        pred_month.append(np.mean(np.array(test_y_pred[1634,:])[(x[i]-x[0]).days:(x[i]-x[0]).days+31]))\n",
    "        vari_month.append(np.sum(np.array(Z_var)[(x[i]-x[0]).days:(x[i]-x[0]).days+31])/np.square(31))\n",
    "    else:\n",
    "        true_month.append(np.mean(np.array(test_y_true[1634,:])[(x[i]-x[0]).days:(x[i]-x[0]).days+calendar.mdays[(i+1)%12]]))\n",
    "        pred_month.append(np.mean(np.array(test_y_pred[1634,:])[(x[i]-x[0]).days:(x[i]-x[0]).days+calendar.mdays[(i+1)%12]]))\n",
    "        vari_month.append(np.sum(np.array(Z_var)[(x[i]-x[0]).days:(x[i]-x[0]).days+calendar.mdays[(i+1)%12]])/np.square(30))\n",
    "pred_month_upper=pred_month+1.96*np.sqrt(vari_month)\n",
    "pred_month_lower=pred_month-1.96*np.sqrt(vari_month)\n",
    "plt.title('Average monthly temperature')\n",
    "plt.plot(y,true_month,linewidth = 3,color=\"orangered\",label='Test Data')\n",
    "plt.plot(y,pred_month,linewidth = 3,color=\"dodgerblue\",label='Prediction')\n",
    "plt.fill_between(y,pred_month_lower,pred_month_upper,color=\"dodgerblue\",alpha=0.3,label='95% PI')\n",
    "ax=plt.gca()\n",
    "plt.axvline(15.6,linewidth = 3,linestyle = \"dashed\",color=\"black\")\n",
    "ax.set_ylabel(r'Temperature ($^{\\circ}$C)')\n",
    "ax.set_xlabel('Time')\n",
    "plt.legend(loc = 'upper right')\n",
    "\n",
    "# plt.savefig('timeseries_nonGaussian.pdf')\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(24,10),gridspec_kw={'width_ratios': [1, 1.2]})\n",
    "\n",
    "x = y = range(105)\n",
    "vmin = np.array(simu_map).min() \n",
    "vmax = np.array(simu_map).max() \n",
    "\n",
    "\n",
    "colors = [\"#48186a\", \"#424086\", \"#33638d\", \"#26828e\", \"#1fa088\", \"#3fbc73\", \"#84d44b\", \"#d8e219\", \"#fcae1e\",'#f51111']\n",
    "levels = len(colors)-1\n",
    "cm     = matplotlib.colors.ListedColormap(colors)\n",
    "\n",
    "contourf1=ax[0].contourf(x,y,np.array(true_map),levels=levels, cmap=cm, linestyles='None', vmin=vmin, vmax=vmax)\n",
    "ax[0].scatter(loc_x, loc_y, c ='black', marker='+',linewidth=4, s=300)\n",
    "# cbar = fig.colorbar(contourf,label='Temperature ($^{\\circ}$C)')\n",
    "ax[0].set_title('True map')\n",
    "ax[0].set_xlim(0,100)\n",
    "ax[0].set_ylim(0,100)\n",
    "\n",
    "\n",
    "vmin = np.array(simu_map).min() \n",
    "vmax = np.array(simu_map).max() \n",
    "\n",
    "colors = [\"#48186a\", \"#424086\", \"#33638d\", \"#26828e\", \"#1fa088\", \"#3fbc73\", \"#84d44b\", \"#d8e219\", \"#fcae1e\",'#f51111']\n",
    "levels = len(colors)-1\n",
    "cm     = matplotlib.colors.ListedColormap(colors)\n",
    "\n",
    "\n",
    "contourf2=ax[1].contourf(x,y,np.array(simu_map),levels=levels, cmap=cm, linestyles='None', vmin=vmin, vmax=vmax)\n",
    "cbar = fig.colorbar(contourf2,label='Temperature ($^{\\circ}$C)')\n",
    "ax[1].scatter(loc_x, loc_y, c ='black', marker='+',linewidth=4, s=300)\n",
    "ax[1].set_title('Predicted map')\n",
    "ax[1].set_xlim(0,100)\n",
    "ax[1].set_ylim(0,100)\n",
    "\n",
    "# plt.savefig('map_nonGaussian.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute contribution coefficient\n",
    "\n",
    "phi=torch.diag(s) @ vt\n",
    "mean=torch.mean(phi, dim=1)\n",
    "gamma=train_att_best\n",
    "gamma=gamma.T\n",
    "Wemb=paras[0]\n",
    "W=paras[17]\n",
    "b=paras[18]\n",
    "\n",
    "def calc_spt_coef(i,j):\n",
    "    return((((gamma[:,i]*Wemb[:,j]).reshape(1,64)@W.T)@mean.reshape(1095,1))+train_coef_best[i,j]).detach().cpu().numpy()\n",
    "\n",
    "contribution=[[] for i in range(11)]\n",
    "for j in trange(11):\n",
    "    for i in range(7025):\n",
    "        contribution[j].append(calc_spt_coef(i,j))\n",
    "\n",
    "res=[]\n",
    "for i in range(11):\n",
    "    res.append(np.mean([j for j in contribution[i] if j!=0]))\n",
    "res[1:]=res[1:]-min(res[1:])\n",
    "print('Contribution coefficient:',res)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
