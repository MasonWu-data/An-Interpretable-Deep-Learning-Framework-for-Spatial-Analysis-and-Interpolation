{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import matplotlib\n",
    "import torch\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.init as init\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.utils.data as Data\n",
    "import matplotlib.dates as mdates\n",
    "from torch.autograd import Variable\n",
    "import time\n",
    "import datetime\n",
    "import sklearn.metrics as sk\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import calendar\n",
    "import numpy as np\n",
    "import joblib\n",
    "import ast\n",
    "import scipy as sp\n",
    "import scipy.stats as st\n",
    "import scipy.optimize as opt\n",
    "from numpy.core.fromnumeric import trace\n",
    "from numpy.linalg import inv\n",
    "from scipy.optimize import root\n",
    "import random\n",
    "from scipy.stats import norm\n",
    "from tkinter import _flatten\n",
    "import sklearn.metrics as sk\n",
    "from tqdm.notebook import trange, tqdm\n",
    "from scipy.stats import wishart\n",
    "from scipy.stats import invwishart\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "from tqdm.notebook import trange, tqdm\n",
    "\n",
    "rng = np.random.default_rng()\n",
    "device0 = torch.device('cuda' if torch.cuda.is_available else 'cpu')\n",
    "np.random.seed(16)\n",
    "torch.manual_seed(16)\n",
    "torch.cuda.manual_seed(16) \n",
    "torch.set_default_dtype(torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "X_spt=joblib.load('X_cov')\n",
    "Z=joblib.load('Data_nonGaussian')\n",
    "# Z=joblib.load('Data_Gaussian')\n",
    "X_spt=X_spt.float().cuda()\n",
    "Z=Z.float().cuda()\n",
    "T=1095\n",
    "S=11025\n",
    "Z=Z.T\n",
    "Z.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LSTM & GRU\n",
    "# calculate the scaling parameters using only the Training subset\n",
    "Z_scaler=MinMaxScaler(feature_range=(0, 1))\n",
    "Z_scaled_tmp=Z_scaler.fit_transform(torch.transpose(Z[:7025], 0,1).cpu())\n",
    "Z_scaled_tmp=torch.tensor(Z_scaled_tmp).float().cuda().T\n",
    "Z_scaled_tmp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_LSTM=torch.repeat_interleave(X_spt,1095,dim=0)\n",
    "X_LSTM=X_LSTM.reshape(11025,1095,11)\n",
    "\n",
    "Z_max=np.mean(Z_scaler.data_max_)\n",
    "Z_min=np.mean(Z_scaler.data_min_)\n",
    "delta=Z_max-Z_min\n",
    "def rescale(raw):\n",
    "    return(raw*delta+Z_min)\n",
    "\n",
    "class Simu_LSTM(nn.Module): \n",
    "    def __init__(self,dim_output=1, dim_input=11, dim_emb=64, dropout_input=0.8, dropout_emb=0.5, \n",
    "                 dropout_context=0.5,  l2=0.0001, batch_first=True):\n",
    "        super(Simu_LSTM,self).__init__()\n",
    "        self.batch_first = batch_first\n",
    "        self.embedding = nn.Sequential(\n",
    "            nn.Dropout(p=dropout_input),\n",
    "            nn.Linear(dim_input, dim_emb, bias=False),\n",
    "            nn.Dropout(p=dropout_emb)\n",
    "        )\n",
    "        init.xavier_normal_(self.embedding[1].weight)\n",
    "        \n",
    "        # self.rnn = nn.GRU(input_size=dim_emb, hidden_size=dim_emb, num_layers=5, batch_first=self.batch_first,bidirectional=True)\n",
    "        \n",
    "        self.rnn = nn.LSTM(input_size=dim_emb, hidden_size=dim_emb, num_layers=5, batch_first=self.batch_first,bidirectional=True)\n",
    "        \n",
    "        self.attention = nn.Linear(2*dim_emb,2*dim_emb,bias=True)\n",
    "        init.xavier_normal_(self.attention.weight, gain=nn.init.calculate_gain('tanh'))\n",
    "        self.attention.bias.data.zero_()\n",
    "        \n",
    "        \n",
    "        self.output = nn.Sequential(\n",
    "            nn.Dropout(p=dropout_context),\n",
    "            nn.Linear(in_features=2*dim_emb, out_features=dim_output)\n",
    "        )\n",
    "        init.xavier_normal_(self.output[1].weight)\n",
    "        self.output[1].bias.data.zero_()\n",
    "\n",
    "    def forward(self,x):\n",
    "        \n",
    "        emb = self.embedding(x)\n",
    "        \n",
    "        h,_=self.rnn(emb)\n",
    "        \n",
    "        temp=self.output(F.relu(h))\n",
    "        \n",
    "        finaltemp=temp.reshape(temp.shape[0],temp.shape[1])\n",
    "        finaltemp=rescale(finaltemp)\n",
    "        \n",
    "        return finaltemp,h\n",
    "    \n",
    "simulstm = Simu_LSTM().cuda()\n",
    "print(simulstm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = Data.TensorDataset(X_LSTM[:7025,:,:], Z[:7025,:])\n",
    "train_loader=Data.DataLoader(dataset=train_set, batch_size=128,shuffle=False)\n",
    "\n",
    "valid_set = Data.TensorDataset(X_LSTM[7025:9025,:,:] , Z[7025:9025,:])\n",
    "valid_loader=Data.DataLoader(dataset=valid_set, batch_size=256,shuffle=False)\n",
    "\n",
    "test_set = Data.TensorDataset(X_LSTM[9025:,:,:] , Z[9025:,:])\n",
    "test_loader =Data.DataLoader(dataset=test_set, batch_size=256,shuffle=False)\n",
    "\n",
    "optimizer = torch.optim.Adam(simulstm.parameters(),lr = 0.001)\n",
    "loss_func = torch.nn.MSELoss()\n",
    "    \n",
    "def epoch(loader,train=False):\n",
    "    if train:\n",
    "        simulstm.train()\n",
    "        mode = 'Train'\n",
    "    else:\n",
    "        simulstm.eval()\n",
    "        mode = 'Eval'\n",
    "    \n",
    "    true_value = []\n",
    "    outputs = []\n",
    "    att =[]\n",
    "    losses = 0\n",
    "\n",
    "    for step ,(batch_x,batch_z) in enumerate(loader):\n",
    "        input_var=Variable(batch_x)\n",
    "        target_var=Variable(batch_z)\n",
    "    \n",
    "        prediction,att = simulstm(input_var)\n",
    "        loss = loss_func(prediction,target_var)\n",
    "        \n",
    "        true_value.append(batch_z)\n",
    "        outputs.append(prediction.data)\n",
    "        losses=losses+loss.data\n",
    "#         att.append(attention.data)\n",
    "    \n",
    "        # compute gradient and do update step\n",
    "        if train:\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "    return torch.cat(true_value,0),torch.cat(outputs, 0),losses/len(loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main\n",
    "best_valid_epoch = 0\n",
    "best_valid_loss = 1e10\n",
    "\n",
    "\n",
    "train_losses = []\n",
    "valid_losses = []\n",
    "n_epochs=4000\n",
    "\n",
    "time_start=time.time()\n",
    "\n",
    "for ei in trange(n_epochs):\n",
    "            \n",
    "    train_y_true, train_y_pred, train_loss = epoch(train_loader,train=True)\n",
    "    train_losses.append(train_loss)\n",
    "\n",
    "#     Eval\n",
    "    valid_y_true, valid_y_pred, valid_loss = epoch(valid_loader)\n",
    "    valid_losses.append(valid_loss)\n",
    "\n",
    "    valid_y_true = valid_y_true.cpu()\n",
    "    valid_y_pred = valid_y_pred.cpu()\n",
    "\n",
    "\n",
    "    is_best = valid_loss < best_valid_loss\n",
    "\n",
    "    if is_best:\n",
    "        best_valid_epoch = ei\n",
    "        best_valid_loss = valid_loss\n",
    "\n",
    "\n",
    "    # evaluate on the test set\n",
    "        test_y_true,test_y_pred,  test_loss = epoch(test_loader)\n",
    "        \n",
    "        train_y_true_best = train_y_true.cpu()\n",
    "        train_y_pred_best = train_y_pred.cpu()\n",
    "        test_y_true = test_y_true.cpu()\n",
    "        test_y_pred = test_y_pred.cpu()\n",
    "        \n",
    "        \n",
    "        # with open('GRU_case.txt', 'w') as f:\n",
    "        #     f.write('Best Validation Epoch: {}\\n'.format(ei))\n",
    "        #     f.write('Best Validation Loss: {}\\n'.format(best_valid_loss))\n",
    "        #     f.write('Train Loss: {}\\n'.format(train_loss))\n",
    "        #     f.write('Test Loss: {}\\n'.format(test_loss))\n",
    "\n",
    "        with open('LSTM_simu.txt', 'w') as f:\n",
    "            f.write('Best Validation Epoch: {}\\n'.format(ei))\n",
    "            f.write('Best Validation Loss: {}\\n'.format(best_valid_loss))\n",
    "            f.write('Train Loss: {}\\n'.format(train_loss))\n",
    "            f.write('Test Loss: {}\\n'.format(test_loss))\n",
    "\n",
    "   \n",
    "        torch.save(simulstm,'LSTM_simu.pt')\n",
    "        \n",
    "time_end=time.time()\n",
    "print('Run Time (s):' ,time_end-time_start)\n",
    "print('Run Time per Epoch (s):'(time_end-time_start)/n_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DSTM\n",
    "X_spt=joblib.load('X_cov')\n",
    "Z=joblib.load('Data_nonGaussian')\n",
    "X_spt=X_spt.float().cuda()\n",
    "Z=Z.float().cuda()\n",
    "T=1095\n",
    "S=50\n",
    "Z_tmp=np.array(Z.cpu())\n",
    "Z=[]\n",
    "for i in Z_tmp[:,:50]:\n",
    "    Z.append(np.c_[i])\n",
    "X_cov=np.array(X_spt.cpu())[:50,:]\n",
    "l=np.array([[1] for i in range(50)])\n",
    "X_cov=np.concatenate((l,X_cov),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sam_beta0(set_beta,set_omega,set_g):\n",
    "    V0=inv((set_g[-1]**2)*inv(set_omega[-1])+np.identity(len(sample_beta[0][0])))\n",
    "    a0=set_g[-1]*(inv(set_omega[-1])@set_beta[-1][0])+np.c_[[0.5 for i in range(11)]]\n",
    "    new=rng.multivariate_normal(V0@a0.flatten(), V0, (1,),'raise',method='cholesky')\n",
    "    return(new.T)\n",
    "\n",
    "def sam_beta(t,set_beta0,set_beta,set_betaT,set_nu,set_omega,set_g):\n",
    "    Vt=inv(X_cov.T@inv(set_nu[-1])@X_cov+(1+set_g[-1]**2)*inv(set_omega[-1]))\n",
    "    if t==1:\n",
    "        at=X_cov.T@inv(set_nu[-1])@Z[t-1]+set_g[-1]*inv(set_omega[-1])@set_beta0[-1]+set_g[-1]*inv(set_omega[-1])@set_beta[-2][t]\n",
    "    elif t==T-1:\n",
    "        at=X_cov.T@inv(set_nu[-1])@Z[t-1]+set_g[-1]*inv(set_omega[-1])@set_beta[-1][t-2]+set_g[-1]*inv(set_omega[-1])@set_betaT[-1]\n",
    "    else:\n",
    "        at=X_cov.T@inv(set_nu[-1])@Z[t-1]+set_g[-1]*inv(set_omega[-1])@set_beta[-1][t-2]+set_g[-1]*inv(set_omega[-1])@set_beta[-2][t]\n",
    "    new=rng.multivariate_normal(Vt@at.flatten(), Vt, (1,),'raise',method='cholesky')\n",
    "    return(new.T)\n",
    "    \n",
    "def sam_betaT(set_beta,set_nu,set_omega,set_g):\n",
    "    VT=inv(X_cov.T@inv(set_nu[-1])@X_cov+inv(set_omega[-1]))\n",
    "    aT=X_cov.T@inv(set_nu[-1])@Z[-1]+set_g[-1]*inv(set_omega[-1])@set_beta[-1][-1]\n",
    "    new=rng.multivariate_normal(VT@aT.flatten(), VT, (1,),'raise',method='cholesky')\n",
    "    return(new.T)\n",
    "\n",
    "def sam_nu(set_beta,set_betaT,set_g):\n",
    "    tmp=0\n",
    "    fulldata=[(Z[i]-X_cov@set_beta[-1][i]) for i in range(T-1) if len(Z[i])==S]\n",
    "    for i in fulldata:\n",
    "        tmp=tmp+i@i.T\n",
    "    tmp=tmp+(Z[-1]-X_cov@set_betaT[-1])@(Z[-1]-X_cov@set_betaT[-1]).T\n",
    "    new=wishart.rvs(df=n_nu+len(fulldata)+1, scale=inv(tmp+V_nu), size=1, random_state=None)\n",
    "    return(inv(new))\n",
    "    \n",
    "def sam_omega(set_beta0,set_beta,set_betaT,set_g):\n",
    "    tmp=0\n",
    "    fullcov=[(set_beta[-1][i]-set_g[-1]*set_beta[-1][i-1]) for i in range(1,T-1)]\n",
    "    fullcov.append(set_beta[-1][0]-set_g[-1]*set_beta0[-1])\n",
    "    fullcov.append(set_betaT[-1]-set_g[-1]*set_beta[-1][-1])\n",
    "    for i in fullcov:\n",
    "        tmp=tmp+i@i.T\n",
    "    new=wishart.rvs(df=n_omega+T, scale=inv(tmp+V_omega), size=1, random_state=None)\n",
    "    return(inv(new))\n",
    "\n",
    "def sam_g(set_beta0,set_beta,set_betaT,set_omega):\n",
    "    new=-1\n",
    "    vg=np.sum([set_beta[-1][i].T@inv(set_omega[-1])@set_beta[-1][i] for i in range(0,T-1)])\n",
    "    vg=vg+set_beta0[-1].T@inv(set_omega[-1])@set_beta0[-1]\n",
    "    ag=np.sum([set_beta[-1][i+1].T@inv(set_omega[-1])@set_beta[-1][i] for i in range(0,T-2)])\n",
    "    ag=ag+set_betaT[-1].T@inv(set_omega[-1])@set_beta[-1][-1]\n",
    "    ag=ag+set_beta[-1][0].T@inv(set_omega[-1])@set_beta0[-1]\n",
    "    while new<=0 or new>=1:\n",
    "        new=rng.normal(loc=ag/vg,scale=np.sqrt(1/vg))\n",
    "    return(new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#main Gibbs\n",
    "T=1095\n",
    "S=50\n",
    "N_gibbs=40000\n",
    "n_nu=S+10\n",
    "n_omega=11+10\n",
    "V_nu=np.identity(S)/(n_nu-S-1)\n",
    "V_omega=np.identity(11)/(n_omega-12)\n",
    "sample_beta0=[rng.multivariate_normal([0.5 for i in range(11)],np.identity(11),(1,),'raise',method='cholesky').T]\n",
    "sample_betaT=[np.c_[rng.uniform(0.001,1,size=11)]]\n",
    "sample_nu=[invwishart.rvs(df=n_nu, scale=(n_nu-S-1)*np.identity(S), size=1, random_state=None)]\n",
    "sample_omega=[invwishart.rvs(df=n_omega, scale=(n_omega-12)*np.identity(11), size=1, random_state=None)]\n",
    "sample_g=[rng.uniform(0.001,1)]\n",
    "sample_beta=[[np.c_[rng.uniform(0.001,1,size=11)] for i in range(T-1)]]\n",
    "\n",
    "time_start=time.time()\n",
    "while(len(sample_beta0))<2:\n",
    "    if len(sample_beta0)==1:\n",
    "        print('Sampling Start')\n",
    "    if len(sample_beta0)==10000:\n",
    "        print('25% finished')\n",
    "    elif len(sample_beta0)==20000:\n",
    "        print('50% finished')\n",
    "    elif len(sample_beta0)==30000:\n",
    "        print('75% finished')\n",
    "    sample_beta0.append(sam_beta0(sample_beta,sample_omega,sample_g))\n",
    "    sample_beta.append([np.c_[rng.uniform(0.001,1,size=11)] for i in range(T-1)])\n",
    "    for t in range(1,T):\n",
    "        sample_beta[-1][t-1]=sam_beta(t,sample_beta0,sample_beta,sample_betaT,sample_nu,sample_omega,sample_g)\n",
    "    sample_betaT.append(sam_betaT(sample_beta,sample_nu,sample_omega,sample_g))\n",
    "    sample_nu.append(sam_nu(sample_beta,sample_betaT,sample_g))\n",
    "    sample_omega.append(sam_omega(sample_beta0,sample_beta,sample_betaT,sample_g))\n",
    "    sample_g.append(sam_g(sample_beta0,sample_beta,sample_betaT,sample_omega))\n",
    "time_end=time.time()\n",
    "joblib.dump(sample_beta0,'sample_beta0')\n",
    "joblib.dump(sample_beta,'sample_beta')\n",
    "joblib.dump(sample_betaT,'sample_betaT')\n",
    "joblib.dump(sample_nu,'sample_nu')\n",
    "joblib.dump(sample_omega,'sample_omega')\n",
    "joblib.dump(sample_g,'sample_g')\n",
    "print('Run Time:' ,time_end-time_start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta0sample=[i[0][0] for i in sample_beta0[int(0.5*N_gibbs):]]\n",
    "beta0burnin=[i[0][0] for i in sample_beta0[0:int(0.5*N_gibbs)]]\n",
    "betasample=[i for i in sample_beta[int(0.5*N_gibbs):]]\n",
    "betaburnin=[i for i in sample_beta[0:int(0.5*N_gibbs)]]\n",
    "beta_avg=np.mean(betasample,axis=0)\n",
    "\n",
    "betaTsample=[i for i in sample_betaT[int(0.5*N_gibbs):]]\n",
    "betaTburnin=[i for i in sample_betaT[0:int(0.5*N_gibbs)]]\n",
    "betaT_avg=np.mean(betaTsample,axis=0)\n",
    "betaT_avg=betaT_avg.reshape(1,12,1)\n",
    "beta_avg=np.concatenate((beta_avg,betaT_avg),axis=0)\n",
    "\n",
    "gsample=[i[0][0] for i in sample_g[int(0.5*N_gibbs):]]\n",
    "gburnin=[i[0][0] for i in sample_g[1:int(0.5*N_gibbs)]]\n",
    "gburnin.insert(0,sample_g[0])\n",
    "g_burnin_avg=[np.mean(gburnin[:i]) for i in range(1,len(beta0burnin)+1)]\n",
    "g_sample_avg=[np.mean(gsample[:i]) for i in range(1,len(beta0sample)+1)]\n",
    "\n",
    "nusample=[i for i in sample_nu[int(0.5*N_gibbs):]]\n",
    "nuburnin=[i for i in sample_nu[0:int(0.5*N_gibbs)]]\n",
    "nusample_avg=np.sum(nusample,axis=0)/len(nusample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute MSE\n",
    "Z_pred=[]\n",
    "for t in range(1095):\n",
    "    tmp=X_cov@beta_avg[t]+rng.multivariate_normal([0 for i in range(50)],nusample_avg,(1,),'raise',method='cholesky').T\n",
    "    Z_pred.append(np.c_[tmp])\n",
    "print('Training MSE:',sk.mean_squared_error(np.array(Z_pred).flatten(),np.array(Z).flatten()))\n",
    "\n",
    "X_test=np.array(X_spt.cpu())[50:100,:]\n",
    "X_test=np.concatenate((l,X_test),axis=1)\n",
    "Z_test=[]\n",
    "for i in Z_tmp[:,50:100]:\n",
    "    Z_test.append(np.c_[i])\n",
    "Z_pred2=[]\n",
    "\n",
    "for t in range(1095):\n",
    "    tmp=X_test@beta_avg[t]+rng.multivariate_normal([0 for i in range(50)],nusample_avg,(1,),'raise',method='cholesky').T\n",
    "    Z_pred2.append(np.c_[tmp])\n",
    "print('Test MSE:',sk.mean_squared_error(np.array(Z_pred2).flatten(),np.array(Z_test).flatten()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Regression Coefficient\n",
    "for i in range(12):\n",
    "    print('Covariate',i,':',np.mean(beta_avg,axis=0)[i])\n",
    "print('------------')\n",
    "for i in range(2,12):\n",
    "    print('Covariate',i,':',np.mean(beta_avg,axis=0)[i]-np.mean(beta_avg,axis=0)[2])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
